#!/usr/bin/env python3
"""
ã‚µãƒ¼ãƒãƒ¼å…¨ä½“ã§ã®GPUé–“ãƒ‡ãƒ¼ã‚¿é€šä¿¡ã®åŒ…æ‹¬çš„æ¤œè¨¼
ã‚³ãƒ³ãƒ†ãƒŠç’°å¢ƒã§ã‚‚å‹•ä½œã™ã‚‹ã‚ˆã†è¨­è¨ˆ
"""

import os
import sys
import time
import subprocess
import multiprocessing as mp
from pathlib import Path
import json

def check_environment():
    """ç’°å¢ƒã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯"""
    print("=== ç’°å¢ƒåŸºæœ¬ãƒã‚§ãƒƒã‚¯ ===")
    
    # CUDAç’°å¢ƒå¤‰æ•°
    cuda_vars = ['CUDA_VISIBLE_DEVICES', 'CUDA_DEVICE_ORDER', 'CUDA_LAUNCH_BLOCKING']
    for var in cuda_vars:
        value = os.environ.get(var, 'Not set')
        print(f"{var}: {value}")
    
    # ãƒ‡ãƒã‚¤ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
    device_count = 0
    for i in range(8):
        device_path = f'/dev/nvidia{i}'
        if os.path.exists(device_path):
            device_count += 1
            stat = os.stat(device_path)
            print(f"GPU {i}: {device_path} (major={os.major(stat.st_rdev)}, minor={os.minor(stat.st_rdev)})")
    
    print(f"æ¤œå‡ºã•ã‚ŒãŸGPUãƒ‡ãƒã‚¤ã‚¹æ•°: {device_count}")
    return device_count

def test_cuda_runtime():
    """CUDAãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®å‹•ä½œç¢ºèª"""
    print("\n=== CUDAãƒ©ãƒ³ã‚¿ã‚¤ãƒ ç¢ºèª ===")
    
    try:
        # PyTorch import
        import torch
        print(f"PyTorch version: {torch.__version__}")
        print(f"CUDA compiled version: {torch.version.cuda}")
        print(f"CUDA available: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"CUDA device count: {torch.cuda.device_count()}")
            print(f"Current device: {torch.cuda.current_device()}")
            
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                print(f"GPU {i}: {props.name}, Memory: {props.total_memory/1024**3:.1f}GB")
            
            return True, torch.cuda.device_count()
        else:
            print("CUDA not available in PyTorch")
            return False, 0
            
    except ImportError as e:
        print(f"PyTorch import error: {e}")
        return False, 0
    except Exception as e:
        print(f"CUDA runtime error: {e}")
        return False, 0

def force_cuda_initialization():
    """CUDAåˆæœŸåŒ–ã®å¼·åˆ¶å®Ÿè¡Œ"""
    print("\n=== CUDAå¼·åˆ¶åˆæœŸåŒ– ===")
    
    # ç’°å¢ƒå¤‰æ•°è¨­å®š
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6'
    
    try:
        import torch
        
        # å„GPUã§ç°¡å˜ãªè¨ˆç®—ã‚’å®Ÿè¡Œ
        available_gpus = []
        for i in range(7):
            try:
                torch.cuda.set_device(i)
                # ç°¡å˜ãªãƒ†ãƒ³ã‚½ãƒ«ä½œæˆã¨è¨ˆç®—
                x = torch.tensor([1.0], device=f'cuda:{i}')
                y = x * 2
                result = y.cpu().item()
                print(f"GPU {i}: è¨ˆç®—æˆåŠŸ (çµæœ: {result})")
                available_gpus.append(i)
            except Exception as e:
                print(f"GPU {i}: è¨ˆç®—å¤±æ•— - {e}")
        
        return available_gpus
        
    except Exception as e:
        print(f"å¼·åˆ¶åˆæœŸåŒ–å¤±æ•—: {e}")
        return []

def test_basic_p2p_communication():
    """åŸºæœ¬çš„ãªP2Pé€šä¿¡ãƒ†ã‚¹ãƒˆ"""
    print("\n=== åŸºæœ¬P2Pé€šä¿¡ãƒ†ã‚¹ãƒˆ ===")
    
    try:
        import torch
        
        if not torch.cuda.is_available() or torch.cuda.device_count() < 2:
            print("P2Pé€šä¿¡ãƒ†ã‚¹ãƒˆã«ã¯2ã¤ä»¥ä¸Šã®GPUãŒå¿…è¦ã§ã™")
            return False
        
        device_count = torch.cuda.device_count()
        print(f"åˆ©ç”¨å¯èƒ½GPUæ•°: {device_count}")
        
        # P2Pã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½æ€§ç¢ºèª
        p2p_matrix = []
        for i in range(device_count):
            row = []
            for j in range(device_count):
                if i == j:
                    row.append("Self")
                else:
                    try:
                        torch.cuda.set_device(i)
                        can_access = torch.cuda.can_device_access_peer(i, j)
                        row.append("Yes" if can_access else "No")
                    except Exception as e:
                        row.append("Error")
            p2p_matrix.append(row)
        
        # P2Pè¡Œåˆ—è¡¨ç¤º
        print("P2P Access Matrix:")
        header = "    " + "    ".join([f"GPU{j}" for j in range(device_count)])
        print(header)
        for i, row in enumerate(p2p_matrix):
            print(f"GPU{i}: " + "  ".join([f"{cell:>4}" for cell in row]))
        
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿è»¢é€ãƒ†ã‚¹ãƒˆ
        print("\nå®Ÿéš›ã®P2Pè»¢é€ãƒ†ã‚¹ãƒˆ:")
        test_size = 1024 * 1024  # 1M elements
        
        for src in range(min(2, device_count)):
            for dst in range(min(2, device_count)):
                if src != dst:
                    try:
                        torch.cuda.set_device(src)
                        src_tensor = torch.randn(test_size, device=f'cuda:{src}')
                        
                        start_time = time.time()
                        dst_tensor = src_tensor.to(f'cuda:{dst}')
                        torch.cuda.synchronize()
                        end_time = time.time()
                        
                        transfer_time = end_time - start_time
                        bandwidth = (test_size * 4) / (1024 * 1024 * transfer_time)  # MB/s
                        
                        print(f"GPU{src} -> GPU{dst}: {transfer_time:.4f}s, {bandwidth:.1f} MB/s")
                        
                        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç¢ºèª
                        diff = torch.abs(src_tensor.cpu() - dst_tensor.cpu()).max().item()
                        if diff < 1e-6:
                            print(f"  ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§: OK")
                        else:
                            print(f"  ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§: NG (æœ€å¤§å·®åˆ†: {diff})")
                        
                    except Exception as e:
                        print(f"GPU{src} -> GPU{dst}: è»¢é€å¤±æ•— - {e}")
        
        return True
        
    except Exception as e:
        print(f"P2Pé€šä¿¡ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def test_multi_gpu_synchronization():
    """ãƒãƒ«ãƒGPUåŒæœŸãƒ†ã‚¹ãƒˆ"""
    print("\n=== ãƒãƒ«ãƒGPUåŒæœŸãƒ†ã‚¹ãƒˆ ===")
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("CUDA not available")
            return False
        
        device_count = torch.cuda.device_count()
        if device_count < 2:
            print("åŒæœŸãƒ†ã‚¹ãƒˆã«ã¯2ã¤ä»¥ä¸Šã®GPUãŒå¿…è¦ã§ã™")
            return False
        
        # å„GPUã§åŒæ™‚è¨ˆç®—
        tensors = []
        matrix_size = 1024
        
        print(f"å„GPU({device_count}å°)ã§{matrix_size}x{matrix_size}è¡Œåˆ—ä¹—ç®—ã‚’å®Ÿè¡Œ...")
        
        start_time = time.time()
        
        # å„GPUã§è¡Œåˆ—ä¹—ç®—é–‹å§‹
        for i in range(device_count):
            torch.cuda.set_device(i)
            
            # ãƒ©ãƒ³ãƒ€ãƒ è¡Œåˆ—ç”Ÿæˆ
            a = torch.randn(matrix_size, matrix_size, device=f'cuda:{i}')
            b = torch.randn(matrix_size, matrix_size, device=f'cuda:{i}')
            
            # éåŒæœŸã§è¡Œåˆ—ä¹—ç®—é–‹å§‹
            c = torch.matmul(a, b)
            tensors.append((i, c))
        
        # å…¨GPUåŒæœŸå¾…ã¡
        for i in range(device_count):
            torch.cuda.set_device(i)
            torch.cuda.synchronize()
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"å…¨GPUè¨ˆç®—å®Œäº†æ™‚é–“: {total_time:.4f}ç§’")
        print(f"ç†è«–çš„ã‚·ãƒ³ã‚°ãƒ«GPUæ™‚é–“: {total_time * device_count:.4f}ç§’")
        print(f"ä¸¦åˆ—åŠ¹ç‡: {(total_time * device_count) / total_time / device_count * 100:.1f}%")
        
        # çµæœæ¤œè¨¼
        for i, tensor in tensors:
            if torch.isnan(tensor).any() or torch.isinf(tensor).any():
                print(f"GPU{i}: è¨ˆç®—çµæœã«ç•°å¸¸å€¤ã‚ã‚Š")
                return False
            else:
                mean_val = tensor.mean().item()
                print(f"GPU{i}: è¨ˆç®—æˆåŠŸ (å¹³å‡å€¤: {mean_val:.6f})")
        
        return True
        
    except Exception as e:
        print(f"ãƒãƒ«ãƒGPUåŒæœŸãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def test_memory_pressure():
    """ãƒ¡ãƒ¢ãƒªåœ§è¿«ä¸‹ã§ã®GPUé–“é€šä¿¡ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ãƒ¡ãƒ¢ãƒªåœ§è¿«ä¸‹é€šä¿¡ãƒ†ã‚¹ãƒˆ ===")
    
    try:
        import torch
        
        if not torch.cuda.is_available() or torch.cuda.device_count() < 2:
            print("ãƒ†ã‚¹ãƒˆã«ã¯2ã¤ä»¥ä¸Šã®GPUãŒå¿…è¦ã§ã™")
            return False
        
        device_count = torch.cuda.device_count()
        
        # å„GPUã§ãƒ¡ãƒ¢ãƒªã®å¤§éƒ¨åˆ†ã‚’å æœ‰
        memory_hogs = []
        for i in range(device_count):
            try:
                torch.cuda.set_device(i)
                
                # åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã®70%ç¨‹åº¦ã‚’å æœ‰
                total_memory = torch.cuda.get_device_properties(i).total_memory
                target_memory = int(total_memory * 0.7)
                elements = target_memory // 4  # float32
                
                hog = torch.randn(elements, device=f'cuda:{i}')
                memory_hogs.append((i, hog))
                
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                print(f"GPU{i}: {allocated:.1f}GB å æœ‰")
                
            except Exception as e:
                print(f"GPU{i}: ãƒ¡ãƒ¢ãƒªå æœ‰å¤±æ•— - {e}")
        
        # ãƒ¡ãƒ¢ãƒªåœ§è¿«ä¸‹ã§ã®P2Pè»¢é€
        print("ãƒ¡ãƒ¢ãƒªåœ§è¿«ä¸‹ã§ã®P2Pè»¢é€ãƒ†ã‚¹ãƒˆ:")
        test_size = 10 * 1024 * 1024  # 10M elements = 40MB
        
        for src in range(min(2, device_count)):
            for dst in range(min(2, device_count)):
                if src != dst:
                    try:
                        torch.cuda.set_device(src)
                        test_tensor = torch.randn(test_size, device=f'cuda:{src}')
                        
                        start_time = time.time()
                        transferred = test_tensor.to(f'cuda:{dst}')
                        torch.cuda.synchronize()
                        end_time = time.time()
                        
                        transfer_time = end_time - start_time
                        bandwidth = (test_size * 4) / (1024 * 1024 * transfer_time)
                        
                        print(f"GPU{src} -> GPU{dst}: {transfer_time:.4f}s, {bandwidth:.1f} MB/s")
                        
                    except Exception as e:
                        print(f"GPU{src} -> GPU{dst}: è»¢é€å¤±æ•— - {e}")
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        for i, hog in memory_hogs:
            del hog
            torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"ãƒ¡ãƒ¢ãƒªåœ§è¿«ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def test_collective_operations():
    """é›†åˆæ¼”ç®—ãƒ†ã‚¹ãƒˆï¼ˆç–‘ä¼¼åˆ†æ•£ç’°å¢ƒï¼‰"""
    print("\n=== é›†åˆæ¼”ç®—ãƒ†ã‚¹ãƒˆ ===")
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("CUDA not available")
            return False
        
        device_count = torch.cuda.device_count()
        if device_count < 2:
            print("é›†åˆæ¼”ç®—ãƒ†ã‚¹ãƒˆã«ã¯2ã¤ä»¥ä¸Šã®GPUãŒå¿…è¦ã§ã™")
            return False
        
        # å„GPUã«ç•°ãªã‚‹å€¤ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ
        tensors = []
        test_size = 1024 * 1024
        
        for i in range(device_count):
            torch.cuda.set_device(i)
            tensor = torch.full((test_size,), float(i + 1), device=f'cuda:{i}')
            tensors.append(tensor)
        
        print("ç–‘ä¼¼AllReduceå®Ÿè£…ï¼ˆæ‰‹å‹•reduceï¼‰:")
        
        # GPU0ã«å…¨ã¦ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’é›†ç´„
        torch.cuda.set_device(0)
        sum_tensor = torch.zeros(test_size, device='cuda:0')
        
        start_time = time.time()
        
        for i, tensor in enumerate(tensors):
            if i == 0:
                sum_tensor += tensor
            else:
                sum_tensor += tensor.to('cuda:0')
        
        # çµæœã‚’å„GPUã«é…å¸ƒ
        result_tensors = []
        for i in range(device_count):
            if i == 0:
                result_tensors.append(sum_tensor.clone())
            else:
                result_tensors.append(sum_tensor.to(f'cuda:{i}'))
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        total_time = end_time - start_time
        data_size = test_size * 4 * device_count  # float32 * device_count
        bandwidth = data_size / (1024 * 1024 * total_time)
        
        print(f"ç–‘ä¼¼AllReduceå®Œäº†æ™‚é–“: {total_time:.4f}ç§’")
        print(f"ãƒ‡ãƒ¼ã‚¿è»¢é€é‡: {data_size / 1024 / 1024:.1f} MB")
        print(f"å®ŸåŠ¹å¸¯åŸŸå¹…: {bandwidth:.1f} MB/s")
        
        # çµæœæ¤œè¨¼
        expected_sum = sum(range(1, device_count + 1))
        for i, result_tensor in enumerate(result_tensors):
            actual_value = result_tensor[0].item()
            if abs(actual_value - expected_sum) < 1e-6:
                print(f"GPU{i}: AllReduceçµæœæ­£å¸¸ (å€¤: {actual_value})")
            else:
                print(f"GPU{i}: AllReduceçµæœç•°å¸¸ (æœŸå¾…: {expected_sum}, å®Ÿéš›: {actual_value})")
                return False
        
        return True
        
    except Exception as e:
        print(f"é›†åˆæ¼”ç®—ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def generate_test_report(results):
    """ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    print("\n" + "="*60)
    print("ã‚µãƒ¼ãƒãƒ¼çµ±åˆGPUé€šä¿¡æ©Ÿèƒ½æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ")
    print("="*60)
    
    total_tests = len(results)
    passed_tests = sum(1 for result in results.values() if result)
    
    print(f"å®Ÿè¡Œãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
    print(f"æˆåŠŸãƒ†ã‚¹ãƒˆæ•°: {passed_tests}")
    print(f"æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
    
    print("\nå€‹åˆ¥ãƒ†ã‚¹ãƒˆçµæœ:")
    for test_name, result in results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {test_name}: {status}")
    
    print("\nç·åˆè©•ä¾¡:")
    if passed_tests == total_tests:
        print("ğŸŸ¢ å„ªç§€: å…¨ã¦ã®GPUé–“é€šä¿¡æ©Ÿèƒ½ãŒæ­£å¸¸ã«å‹•ä½œ")
    elif passed_tests >= total_tests * 0.8:
        print("ğŸŸ¡ è‰¯å¥½: å¤§éƒ¨åˆ†ã®GPUé–“é€šä¿¡æ©Ÿèƒ½ãŒæ­£å¸¸ã«å‹•ä½œ")
    elif passed_tests >= total_tests * 0.5:
        print("ğŸŸ  æ³¨æ„: ä¸€éƒ¨ã®GPUé–“é€šä¿¡æ©Ÿèƒ½ã«å•é¡Œã‚ã‚Š")
    else:
        print("ğŸ”´ è­¦å‘Š: GPUé–“é€šä¿¡æ©Ÿèƒ½ã«é‡å¤§ãªå•é¡Œã‚ã‚Š")
    
    print("\næ¨å¥¨äº‹é …:")
    if not results.get('basic_p2p', True):
        print("- P2Pé€šä¿¡ãŒå¤±æ•—ã—ã¦ã„ã‚‹å ´åˆã€NVLinkã‚„PCIeæ¥ç¶šã‚’ç¢ºèª")
    if not results.get('multi_gpu_sync', True):
        print("- ãƒãƒ«ãƒGPUåŒæœŸã®å•é¡ŒãŒã‚ã‚‹å ´åˆã€ãƒ‰ãƒ©ã‚¤ãƒãƒ¼æ›´æ–°ã‚’æ¤œè¨")
    if not results.get('memory_pressure', True):
        print("- ãƒ¡ãƒ¢ãƒªåœ§è¿«ä¸‹ã§ã®å•é¡ŒãŒã‚ã‚‹å ´åˆã€ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚’æœ€é©åŒ–")
    if not results.get('collective_ops', True):
        print("- é›†åˆæ¼”ç®—ã®å•é¡ŒãŒã‚ã‚‹å ´åˆã€NCCLè¨­å®šã‚’ç¢ºèª")

def main():
    print("ã‚µãƒ¼ãƒãƒ¼çµ±åˆGPUé–“ãƒ‡ãƒ¼ã‚¿é€šä¿¡åŒ…æ‹¬æ¤œè¨¼")
    print("="*60)
    
    # ç’°å¢ƒãƒã‚§ãƒƒã‚¯
    device_count = check_environment()
    
    if device_count == 0:
        print("GPUãƒ‡ãƒã‚¤ã‚¹ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã€‚æ¤œè¨¼ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return
    
    # CUDAãƒ©ãƒ³ã‚¿ã‚¤ãƒ ç¢ºèª
    cuda_available, cuda_device_count = test_cuda_runtime()
    
    # çµæœè¨˜éŒ²ç”¨è¾æ›¸
    test_results = {}
    
    if cuda_available:
        print(f"\nCUDAåˆ©ç”¨å¯èƒ½: {cuda_device_count}å°ã®GPUã§æ¤œè¨¼å®Ÿè¡Œ")
        
        # åŸºæœ¬P2Pé€šä¿¡ãƒ†ã‚¹ãƒˆ
        test_results['basic_p2p'] = test_basic_p2p_communication()
        
        # ãƒãƒ«ãƒGPUåŒæœŸãƒ†ã‚¹ãƒˆ
        test_results['multi_gpu_sync'] = test_multi_gpu_synchronization()
        
        # ãƒ¡ãƒ¢ãƒªåœ§è¿«ä¸‹é€šä¿¡ãƒ†ã‚¹ãƒˆ
        test_results['memory_pressure'] = test_memory_pressure()
        
        # é›†åˆæ¼”ç®—ãƒ†ã‚¹ãƒˆ
        test_results['collective_ops'] = test_collective_operations()
        
    else:
        print("\nCUDAåˆ©ç”¨ä¸å¯: å¼·åˆ¶åˆæœŸåŒ–ã‚’è©¦è¡Œ")
        available_gpus = force_cuda_initialization()
        
        if available_gpus:
            print(f"å¼·åˆ¶åˆæœŸåŒ–æˆåŠŸ: GPU {available_gpus} ãŒåˆ©ç”¨å¯èƒ½")
            test_results['force_initialization'] = True
        else:
            print("å¼·åˆ¶åˆæœŸåŒ–ã‚‚å¤±æ•—")
            test_results['force_initialization'] = False
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_test_report(test_results)

if __name__ == "__main__":
    main() 